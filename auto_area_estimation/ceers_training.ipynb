{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習モデルの作成と推論\n",
    "1. 回帰問題: 特定のフィルターのフラックス予測:\n",
    "\n",
    "- 目的変数: 例えば、f200w_flux_aper_1 (F200Wフィルターのフラックス)\n",
    "- 説明変数: 他のフィルターのフラックス（例: f115w_flux_aper_1, f150w_flux_aper_1 など）や天体の形状パラメータ (a_image, b_image など）\n",
    "- モデル: 線形回帰、決定木、ランダムフォレストなど\n",
    "- 推論: 学習済みモデルを使って、新しい天体の他のフィルターのフラックスから f200w_flux_aper_1 を予測してみてください。\n",
    "\n",
    "2. 分類問題: 天体の種類を予測 (簡略化):\n",
    "\n",
    "- 特徴量: 異なるフィルターのフラックスの色情報（上記の色-色図で作成したような特徴量）\n",
    "- ラベル (簡略化): 例えば、特定の色の範囲にある天体を「タイプA」、別の範囲にある天体を「タイプB」のように人為的にラベル付けします。\n",
    "- モデル: ロジスティック回帰、サポートベクターマシン、決定木など\n",
    "- 推論: 学習済みモデルを使って、新しい天体の色情報から「タイプ」を予測してみてください。\n",
    "\n",
    "3. クラスタリング: 天体のグループ分け:\n",
    "- 特徴量: 複数のフィルターのフラックス、形状パラメータ、等級など\n",
    "- アルゴリズム: K-means、階層的クラスタリングなど\n",
    "- 推論: 学習済みモデル（クラスタリングの場合は、データのグループ分け）を使って、新しい天体がどのグループに属するかを調べてください。可視化と組み合わせて、各グループの天体の特徴を分析すると面白いでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メインプロセスのみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Check that MPS is available (Apple Silicon GPU を利用可能か確認)\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "# 特徴量として使用する列名\n",
    "features = [\n",
    "    'f115w_flux_aper_1',\n",
    "    'f150w_flux_aper_1',\n",
    "    'mag_auto',\n",
    "    'a_image',\n",
    "    'b_image',\n",
    "    'peak'\n",
    "]\n",
    "# ターゲット変数 (予測したい変数)\n",
    "target = 'area_auto'\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset = HDF5Dataset(\"ceers.hdf5\", features + [target], features + [target])\n",
    "# DataLoader を使用して、データセットからバッチごとにデータを取り出す (この例ではデータセット全体を1つのバッチとしてロード)\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(\"CPU Count:\", cpu_count)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=len(dataset), \n",
    "    shuffle=True,\n",
    ")\n",
    "# DataLoader から 1 バッチ取得\n",
    "for batch in dataloader:\n",
    "    data = {key: batch[key].numpy() for key in features + [target]}  # 必要に応じて Tensor を numpy に変換\n",
    "    break  # 最初のバッチのみ使用\n",
    "# DataFrameに変換\n",
    "df = pd.DataFrame(data)\n",
    "# 欠損値の処理 (NaN を含む行をデータセットから削除)\n",
    "df = df.dropna()\n",
    "# 特徴量とターゲットに分割 (モデルの入力となる特徴量と予測したいターゲット変数を分離)\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "# データの分割 (トレーニング用とテスト用データに分割。テストデータはモデルの性能評価に使用)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# データの標準化 (特徴量のスケールを揃えることで、モデルの学習を安定させ、性能を向上させる)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# NumPy 配列を PyTorch の Tensor に変換 (PyTorch のモデルで扱えるようにデータ形式を変換)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "# データの確認 (トレーニングデータとテストデータの形状を出力して確認)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "# データの確認 (トレーニングデータとテストデータの形状を出力して確認)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "import torch.nn as nn\n",
    "# 入力特徴量の数 (モデルの入力層のノード数を決定)\n",
    "input_size = X_train.shape[1]\n",
    "# 出力変数の数 (モデルの出力層のノード数を決定。ここでは1つの値を予測)\n",
    "output_size = 1\n",
    "# 線形回帰モデルの定義 (PyTorch の nn.Module を継承してモデルを定義)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # 線形層 (全結合層) を定義。入力サイズと出力サイズを指定\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 順伝播処理。入力 x を線形層に通して出力を得る\n",
    "        return self.linear(x)\n",
    "# モデルのインスタンスを作成 (定義したモデルクラスのオブジェクトを作成)\n",
    "model = LinearRegressionModel(input_size, output_size)\n",
    "\n",
    "# モデルのパラメータ数を表示 (モデルの複雑さを確認)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Number of parameters: {num_params}')\n",
    "# モデルの概要を表示 (モデルの構造を確認)\n",
    "print(model)\n",
    "\n",
    "# 損失関数の定義 (回帰問題では平均二乗誤差 (MSE) を使用)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 最適化アルゴリズムの定義 (モデルのパラメータを最適化するためのアルゴリズムを選択。Adam は一般的な選択肢)\n",
    "learning_rate = 0.01 # 学習率を設定。パラメータの更新幅を制御\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# トレーニングのエポック数 (モデルを学習データで繰り返し学習させる回数)\n",
    "num_epochs = 1000\n",
    "losses = [] # 各エポックの損失を記録するためのリスト\n",
    "\n",
    "# トレーニングループ (指定されたエポック数だけ学習を繰り返す)\n",
    "for epoch in range(num_epochs):\n",
    "    # 順伝播 (forward pass) (モデルに入力を与えて予測値を出力)\n",
    "    outputs = model(X_train)\n",
    "    # 損失の計算 (モデルの予測値と正解ラベルとの誤差を計算)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    # 逆伝播とパラメータ更新 (backward pass and optimization)\n",
    "    optimizer.zero_grad() # 勾配をゼロに初期化 (前のイテレーションで計算された勾配をリセット)\n",
    "    loss.backward() # 勾配の計算 (損失関数に基づいて、各パラメータの勾配を計算)\n",
    "    optimizer.step() # パラメータの更新 (計算された勾配に基づいて、モデルのパラメータを更新)\n",
    "\n",
    "    losses.append(loss.item()) # 現在のエポックの損失値をリストに追加\n",
    "\n",
    "    # 一定間隔で損失を出力して学習の進捗を確認\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデル全体の保存 (学習済みモデルのパラメータをファイルに保存)\n",
    "torch.save(model.state_dict(), 'linear_regression_model.pth')\n",
    "print(\"Model saved as linear_regression_model.pth\")\n",
    "\n",
    "# 学習曲線のプロット (オプション) (エポックごとの損失の推移をグラフで可視化)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPS対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# デバイスの設定 (MPS が利用可能なら使用、そうでなければ CPU を使用)\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    device = mps_device\n",
    "    print(\"MPS device found and will be used.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found and will be used.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found, CPU will be used.\")\n",
    "\n",
    "# 特徴量として使用する列名\n",
    "features = [\n",
    "    'f115w_flux_aper_1',\n",
    "    'f150w_flux_aper_1',\n",
    "    'mag_auto',\n",
    "    'a_image',\n",
    "    'b_image',\n",
    "    'peak'\n",
    "]\n",
    "# ターゲット変数 (予測したい変数)\n",
    "target = 'area_auto'\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset = HDF5Dataset(\"ceers.hdf5\", features + [target], features + [target])\n",
    "# DataLoader を使用して、データセットからバッチごとにデータを取り出す (この例ではデータセット全体を1つのバッチとしてロード)\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(\"CPU Count:\", cpu_count)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=len(dataset),\n",
    "    shuffle=True,\n",
    ")\n",
    "# DataLoader から 1 バッチ取得\n",
    "for batch in dataloader:\n",
    "    data = {key: batch[key].numpy() for key in features + [target]}  # 必要に応じて Tensor を numpy に変換\n",
    "    break  # 最初のバッチのみ使用\n",
    "# DataFrameに変換\n",
    "df = pd.DataFrame(data)\n",
    "# 欠損値の処理 (NaN を含む行をデータセットから削除)\n",
    "df = df.dropna()\n",
    "# 特徴量とターゲットに分割 (モデルの入力となる特徴量と予測したいターゲット変数を分離)\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "# データの分割 (トレーニング用とテスト用データに分割。テストデータはモデルの性能評価に使用)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# データの標準化 (特徴量のスケールを揃えることで、モデルの学習を安定させ、性能を向上させる)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# NumPy 配列を PyTorch の Tensor に変換 (PyTorch のモデルで扱えるようにデータ形式を変換)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "# データの確認 (トレーニングデータとテストデータの形状を出力して確認)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "# データの確認 (トレーニングデータとテストデータの形状を出力して確認)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "import torch.nn as nn\n",
    "# 入力特徴量の数 (モデルの入力層のノード数を決定)\n",
    "input_size = X_train.shape[1]\n",
    "# 出力変数の数 (モデルの出力層のノード数を決定。ここでは1つの値を予測)\n",
    "output_size = 1\n",
    "# 線形回帰モデルの定義 (PyTorch の nn.Module を継承してモデルを定義)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # 線形層 (全結合層) を定義。入力サイズと出力サイズを指定\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 順伝播処理。入力 x を線形層に通して出力を得る\n",
    "        return self.linear(x)\n",
    "# モデルのインスタンスを作成 (定義したモデルクラスのオブジェクトを作成)\n",
    "model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "# モデルのパラメータ数を表示 (モデルの複雑さを確認)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Number of parameters: {num_params}')\n",
    "# モデルの概要を表示 (モデルの構造を確認)\n",
    "print(model)\n",
    "\n",
    "# 損失関数の定義 (回帰問題では平均二乗誤差 (MSE) を使用)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 最適化アルゴリズムの定義 (モデルのパラメータを最適化するためのアルゴリズムを選択。Adam は一般的な選択肢)\n",
    "learning_rate = 0.01 # 学習率を設定。パラメータの更新幅を制御\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# トレーニングのエポック数 (モデルを学習データで繰り返し学習させる回数)\n",
    "num_epochs = 1000\n",
    "losses = [] # 各エポックの損失を記録するためのリスト\n",
    "\n",
    "# トレーニングループ (指定されたエポック数だけ学習を繰り返す)\n",
    "for epoch in range(num_epochs):\n",
    "    # 順伝播 (forward pass) (モデルに入力を与えて予測値を出力)\n",
    "    outputs = model(X_train)\n",
    "    # 損失の計算 (モデルの予測値と正解ラベルとの誤差を計算)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    # 逆伝播とパラメータ更新 (backward pass and optimization)\n",
    "    optimizer.zero_grad() # 勾配をゼロに初期化 (前のイテレーションで計算された勾配をリセット)\n",
    "    loss.backward() # 勾配の計算 (損失関数に基づいて、各パラメータの勾配を計算)\n",
    "    optimizer.step() # パラメータの更新 (計算された勾配に基づいて、モデルのパラメータを更新)\n",
    "\n",
    "    losses.append(loss.item()) # 現在のエポックの損失値をリストに追加\n",
    "\n",
    "    # 一定間隔で損失を出力して学習の進捗を確認\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデル全体の保存 (学習済みモデルのパラメータをファイルに保存)\n",
    "torch.save(model.state_dict(), 'linear_regression_model.pth')\n",
    "print(\"Model saved as linear_regression_model.pth\")\n",
    "\n",
    "# 学習曲線のプロット (オプション) (エポックごとの損失の推移をグラフで可視化)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EPOCH 増加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# デバイスの設定 (MPS が利用可能なら使用、そうでなければ CPU を使用)\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    device = mps_device\n",
    "    print(\"MPS device found and will be used.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found and will be used.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found, CPU will be used.\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 特徴量として使用する列名\n",
    "features = [\n",
    "    'f115w_flux_aper_1',\n",
    "    'f150w_flux_aper_1',\n",
    "    'mag_auto',\n",
    "    'a_image',\n",
    "    'b_image',\n",
    "    'peak'\n",
    "]\n",
    "# ターゲット変数 (予測したい変数)\n",
    "target = 'area_auto'\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset = HDF5Dataset(\"ceers.hdf5\", features + [target], features + [target])\n",
    "# DataLoader を使用して、データセットからバッチごとにデータを取り出す (この例ではデータセット全体を1つのバッチとしてロード)\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(\"CPU Count:\", cpu_count)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=len(dataset),\n",
    "    shuffle=True,\n",
    ")\n",
    "# DataLoader から 1 バッチ取得\n",
    "for batch in dataloader:\n",
    "    data = {key: batch[key].numpy() for key in features + [target]}  # 必要に応じて Tensor を numpy に変換\n",
    "    break  # 最初のバッチのみ使用\n",
    "# DataFrameに変換\n",
    "df = pd.DataFrame(data)\n",
    "# 欠損値の処理 (NaN を含む行をデータセットから削除)\n",
    "df = df.dropna()\n",
    "# 特徴量とターゲットに分割 (モデルの入力となる特徴量と予測したいターゲット変数を分離)\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "# データの分割 (トレーニング用とテスト用データに分割。テストデータはモデルの性能評価に使用)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# データの標準化 (特徴量のスケールを揃えることで、モデルの学習を安定させ、性能を向上させる)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# NumPy 配列を PyTorch の Tensor に変換 (PyTorch のモデルで扱えるようにデータ形式を変換)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "# データの確認 (トレーニングデータとテストデータの形状を出力して確認)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "# データの確認 (トレーニングデータとテストデータの形状を出力して確認)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "import torch.nn as nn\n",
    "# 入力特徴量の数 (モデルの入力層のノード数を決定)\n",
    "input_size = X_train.shape[1]\n",
    "# 出力変数の数 (モデルの出力層のノード数を決定。ここでは1つの値を予測)\n",
    "output_size = 1\n",
    "# 線形回帰モデルの定義 (PyTorch の nn.Module を継承してモデルを定義)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # 線形層 (全結合層) を定義。入力サイズと出力サイズを指定\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 順伝播処理。入力 x を線形層に通して出力を得る\n",
    "        return self.linear(x)\n",
    "# モデルのインスタンスを作成 (定義したモデルクラスのオブジェクトを作成)\n",
    "model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "# モデルのパラメータ数を表示 (モデルの複雑さを確認)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Number of parameters: {num_params}')\n",
    "# モデルの概要を表示 (モデルの構造を確認)\n",
    "print(model)\n",
    "\n",
    "# 損失関数の定義 (回帰問題では平均二乗誤差 (MSE) を使用)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 最適化アルゴリズムの定義 (モデルのパラメータを最適化するためのアルゴリズムを選択。Adam は一般的な選択肢)\n",
    "learning_rate = 0.01 # 学習率を設定。パラメータの更新幅を制御\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 目標とする損失値\n",
    "target_loss = 0.01  # 例：目標とする損失値を設定\n",
    "losses = [] # 各エポックの損失を記録するためのリスト\n",
    "epoch = 0\n",
    "max_epochs = 10000 # 無限ループを防ぐための最大エポック数\n",
    "best_loss_file = \"best_loss.txt\"\n",
    "model_save_path = \"linear_regression_model.pth\"\n",
    "best_loss = float('inf')\n",
    "best_loss_achieved = False\n",
    "\n",
    "# 既存のベストロスをファイルからロード\n",
    "if os.path.exists(best_loss_file):\n",
    "    with open(best_loss_file, 'r') as f:\n",
    "        try:\n",
    "            best_loss = float(f.readline())\n",
    "            print(f\"Loaded best loss from file: {best_loss:.4f}\")\n",
    "        except ValueError:\n",
    "            print(\"Could not read best loss from file, starting with infinity.\")\n",
    "\n",
    "# トレーニングループ (目標損失に達するか、最大エポック数に達するまで学習を繰り返す)\n",
    "while True:\n",
    "    # 順伝播 (forward pass) (モデルに入力を与えて予測値を出力)\n",
    "    outputs = model(X_train)\n",
    "    # 損失の計算 (モデルの予測値と正解ラベルとの誤差を計算)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    current_loss = loss.item()\n",
    "\n",
    "    losses.append(current_loss) # 現在のエポックの損失値をリストに追加\n",
    "\n",
    "    # 損失が目標値以下になったらループを抜ける\n",
    "    if current_loss <= target_loss:\n",
    "        print(f'Epoch [{epoch+1}], Loss: {current_loss:.4f} (Target loss reached)')\n",
    "        break\n",
    "\n",
    "    # 現在の損失が過去最高の損失よりも小さい場合にモデルを保存し、ベストロスを更新\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        with open(best_loss_file, 'w') as f:\n",
    "            f.write(str(best_loss))\n",
    "        print(f'Epoch [{epoch+1}], Loss: {current_loss:.4f} (Best loss so far, model saved)')\n",
    "        best_loss_achieved = True\n",
    "\n",
    "    # 逆伝播とパラメータ更新 (backward pass and optimization)\n",
    "    optimizer.zero_grad() # 勾配をゼロに初期化 (前のイテレーションで計算された勾配をリセット)\n",
    "    loss.backward() # 勾配の計算 (損失関数に基づいて、各パラメータの勾配を計算)\n",
    "    optimizer.step() # パラメータの更新 (計算された勾配に基づいて、モデルのパラメータを更新)\n",
    "\n",
    "    # 一定間隔で損失を出力して学習の進捗を確認\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}], Loss: {current_loss:.4f}')\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    # 最大エポック数を超えた場合はループを抜ける (過学習を防ぐためにも重要)\n",
    "    if epoch >= max_epochs:\n",
    "        print(f'Epoch [{epoch+1}], Loss: {current_loss:.4f} (Max epochs reached)')\n",
    "        break\n",
    "\n",
    "print(f\"Training finished. Best loss achieved: {best_loss:.4f}\")\n",
    "\n",
    "# max_epochs に達したが best_loss が更新されなかった場合のメッセージ\n",
    "if epoch >= max_epochs and not best_loss_achieved:\n",
    "    print(\"Maximum epochs reached, but no improvement in best loss. Model not saved (or the initial best loss remains).\")\n",
    "elif best_loss_achieved:\n",
    "    print(f\"Model with best loss saved to {model_save_path}\")\n",
    "else:\n",
    "    # 目標損失に達した場合\n",
    "    print(f\"Model saved to {model_save_path}\") # 目標損失に達した場合も保存されているのでメッセージを表示\n",
    "\n",
    "# モデルを評価モードにする (不要な勾配計算などを抑制)\n",
    "# モデルを評価モードに設定します。これにより、バッチノーマライゼーションやドロップアウトなどの層が評価モードで動作するようになります\n",
    "model.eval()\n",
    "# このブロック内の計算では勾配が計算されません。評価時には不要なため、メモリ使用量を減らし、計算を高速化できます。\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    predicted_values = test_outputs.cpu().numpy()\n",
    "    true_values = y_test.cpu().numpy()\n",
    "\n",
    "# 予測値 vs. 真値の散布図\n",
    "# あなたのモデルがテストデータに対して行った予測値と、そのテストデータの実際の正解の値（真値）を比較するためのもの\n",
    "# 横軸 (True Values): テストデータにおけるターゲット変数（今回の場合は area_auto）の実際の値を示しています\n",
    "# 縦軸 (Predicted Values): あなたのモデルが同じテストデータに対して予測した area_auto の値を示しています。\n",
    "# 理想的な結果: もしモデルが完璧であれば、全ての点は左下から右上に向かう対角線上に並びます。この対角線は「予測値 = 真値」を表しています。\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(true_values, predicted_values)\n",
    "plt.plot([true_values.min(), true_values.max()], [true_values.min(), true_values.max()], 'k--', lw=2) # 理想的な対角線\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs. True Values')\n",
    "plt.show()\n",
    "\n",
    "# 残差プロット\n",
    "# この図は、モデルの予測誤差（残差）がどのように分布しているかを見るためのものです。残差は「予測値 - 真値」で計算されます。\n",
    "# 横軸 (Predicted Values): モデルが予測した値を示しています。\n",
    "# 縦軸 (Residuals): 残差を示しています。残差は「予測値 - 真値」で計算されます。\n",
    "# 残差が 0 のライン (赤い破線): 理想的な場合、残差は 0 であるべきです。このラインは、モデルが完璧に予測した場合の残差を示しています。\n",
    "residuals = predicted_values - true_values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(predicted_values, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--') # 残差が 0 のライン\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals (Predicted - True)')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# 学習曲線のプロット (オプション) (エポックごとの損失の推移をグラフで可視化)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_dataset import HDF5Dataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time  # time モジュールをインポート\n",
    "\n",
    "# Ray Tune のためのインポート\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# デバイスの設定 (Ray Tune の設定で GPU を利用するかどうかを指定できるため、ここでは CPU に固定)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using CPU for Ray Tune.\")\n",
    "\n",
    "# 特徴量として使用する列名\n",
    "features = [\n",
    "    # マグニチュードはフラックスと逆の関係にあるため、暗いオブジェクト（マグニチュードが大きい）ほど、area_auto が小さくなる傾向があるかもしれません。\n",
    "    'mag_auto',\n",
    "    'mag_iso',\n",
    "    # アイソフォタル（等光度）アパーチャによる面積であり、オブジェクトのサイズを測る別の方法です。自動アパーチャによる面積と強い相関があると考えられます。\n",
    "    'area_iso',\n",
    "    # Kron 半径は、オブジェクトの光度分布に基づいて定義される半径で、オブジェクトのサイズを示す指標となります。Kron 半径が大きいほど、area_auto も大きくなる傾向があると考えられます。\n",
    "    'kron_radius',\n",
    "    # これはオブジェクトを構成するピクセルの数です。ピクセル数が多ければ多いほど、オブジェクトの面積も大きくなるため、area_auto と直接的な関係があると考えられます。\n",
    "    'npix',\n",
    "    # これらのパラメータは、画像内でのオブジェクトの範囲を示す最大・最小の x, y 座標です。これらの値からオブジェクトのおおよその面積を推定できるため、area_auto と関連性が高いと考えられます。\n",
    "    'xmax',\n",
    "    'xmin',\n",
    "    'ymax',\n",
    "    'ymin',\n",
    "    # これらのパラメータは、画像内のオブジェクトの形状とサイズに関する二次モーメントです。特に x2_image と y2_image は、オブジェクトの広がりを示すため、area_auto と関連する可能性があります。\n",
    "    'x2_image',\n",
    "    'y2_image',\n",
    "    'xy_image',\n",
    "    # 明るいオブジェクト（フラックスが大きい）ほど、自動検出アルゴリズムによってより広い範囲がオブジェクトとして認識される可能性があり、結果的に area_auto が大きくなることがあります。ただし、点源のようなコンパクトな明るいオブジェクトの場合はこの限りではありません。\n",
    "    'flux_auto',\n",
    "    'flux_iso',\n",
    "    'flux_aper_0',\n",
    "    'flux_aper_1',\n",
    "    'flux_aper_2',\n",
    "    # これらのパラメータは、オブジェクトの全フラックスの一定割合を含む半径を示します。これらの半径が大きいほど、オブジェクトが広がっている可能性があり、area_auto も大きくなる可能性があります。\n",
    "    'flux_radius',\n",
    "    'flux_radius_20',\n",
    "    'flux_radius_90',\n",
    "]\n",
    "# ターゲット変数 (予測したい変数)\n",
    "target = 'area_auto'\n",
    "\n",
    "# 線形回帰モデルの定義 (PyTorch の nn.Module を継承してモデルを定義)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # 線形層 (全結合層) を定義。入力サイズと出力サイズを指定\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 順伝播処理。入力 x を線形層に通して出力を得る\n",
    "        return self.linear(x)\n",
    "\n",
    "# Ray Tune で最適化する train 関数\n",
    "def train_tune(config):\n",
    "    start_time = time.time()  # 学習開始時間を記録\n",
    "\n",
    "    # データセットの読み込み\n",
    "    dataset = HDF5Dataset(config[\"data_path\"], features + [target], None)\n",
    "    dataloader = torch.utils.data.DataLoader( # DataLoader を使用\n",
    "        dataset,\n",
    "        batch_size=len(dataset),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    for batch in dataloader:\n",
    "        data = {key: batch[key].numpy() for key in features + [target]}\n",
    "        break\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.dropna()\n",
    "    # config に基づいて使用する特徴量のリストを作成\n",
    "    selected_features = []\n",
    "    selected_features.append('mag_auto')\n",
    "    if config[\"use_mag_iso\"]:\n",
    "        selected_features.append('mag_iso')\n",
    "    if config[\"use_area_iso\"]:\n",
    "        selected_features.append('area_iso')\n",
    "    if config[\"use_kron_radius\"]:\n",
    "        selected_features.append('kron_radius')\n",
    "    if config[\"use_npix\"]:\n",
    "        selected_features.append('npix')\n",
    "    if config[\"use_xmax\"]:\n",
    "        selected_features.append('xmax')\n",
    "    if config[\"use_xmin\"]:\n",
    "        selected_features.append('xmin')\n",
    "    if config[\"use_ymax\"]:\n",
    "        selected_features.append('ymax')\n",
    "    if config[\"use_ymin\"]:\n",
    "        selected_features.append('ymin')\n",
    "    if config[\"use_x2_image\"]:\n",
    "        selected_features.append('x2_image')\n",
    "    if config[\"use_y2_image\"]:\n",
    "        selected_features.append('y2_image')\n",
    "    if config[\"use_xy_image\"]:\n",
    "        selected_features.append('xy_image')\n",
    "    if config[\"use_flux_auto\"]:\n",
    "        selected_features.append('flux_auto')\n",
    "    if config[\"use_flux_iso\"]:\n",
    "        selected_features.append('flux_iso')\n",
    "    if config[\"use_flux_aper_0\"]:\n",
    "        selected_features.append('flux_aper_0')\n",
    "    if config[\"use_flux_aper_1\"]:\n",
    "        selected_features.append('flux_aper_1')\n",
    "    if config[\"use_flux_aper_2\"]:\n",
    "        selected_features.append('flux_aper_2')\n",
    "    if config[\"use_flux_radius\"]:\n",
    "        selected_features.append('flux_radius')\n",
    "    if config[\"use_flux_radius_20\"]:\n",
    "        selected_features.append('flux_radius_20')\n",
    "    if config[\"use_flux_radius_90\"]:\n",
    "        selected_features.append('flux_radius_90')\n",
    "\n",
    "    X = df[selected_features].values\n",
    "    y = df[target].values\n",
    "    # データの分割 (トレーニング用とテスト用データに分割。テストデータはモデルの性能評価に使用)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # データの標準化 (特徴量のスケールを揃えることで、モデルの学習を安定させ、性能を向上させる)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # NumPy 配列を PyTorch の Tensor に変換 (PyTorch のモデルで扱えるようにデータ形式を変換)\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "\n",
    "    # モデルのインスタンスを作成 (ハイパーパラメータを使用)\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = 1\n",
    "    model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "    # 損失関数とオプティマイザの定義 (ハイパーパラメータを使用)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    best_trial_loss = float('inf')\n",
    "\n",
    "    # トレーニングループ (ハイパーパラメータを使用)\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # 順伝播 (forward pass) (モデルに入力を与えて予測値を出力)\n",
    "        outputs = model(X_train)\n",
    "        # 損失の計算 (モデルの予測値と正解ラベルとの誤差を計算)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        current_loss = loss.item()\n",
    "        # 逆伝播とパラメータ更新 (backward pass and optimization)\n",
    "        optimizer.zero_grad() # 勾配をゼロに初期化 (前のイテレーションで計算された勾配をリセット)\n",
    "        loss.backward() # 勾配の計算 (損失関数に基づいて、各パラメータの勾配を計算)\n",
    "        optimizer.step() # パラメータの更新 (計算された勾配に基づいて、モデルのパラメータを更新)\n",
    "\n",
    "        best_trial_loss = min(best_trial_loss, current_loss)\n",
    "\n",
    "        # Ray Tune に Loss を報告\n",
    "        print({\"loss\": current_loss, \"epoch\": epoch, \"best_loss\": best_trial_loss})\n",
    "        tune.report({\"loss\": current_loss, \"epoch\": epoch, \"best_loss\": best_trial_loss})\n",
    "\n",
    "    end_time = time.time()  # 学習終了時間を記録\n",
    "    duration = end_time - start_time\n",
    "    tune.report({\"loss\": current_loss, \"epoch\": epoch, \"best_loss\": best_trial_loss, \"training_duration\": duration}) # 学習時間を報告\n",
    "\n",
    "# ハイパーパラメータの探索空間を定義\n",
    "# TODO: The number of pre-generated samples (2621440) exceeds\n",
    "# 3 * 2 * 2 * 2 * 2 * 2 = 3 * 2 * 32 = 96 通り\n",
    "config = {\n",
    "    # モデルの学習は、損失関数（モデルの予測と実際のデータのずれを表す関数）の値を最小にするパラメータを見つけるプロセスです\n",
    "    \"learning_rate\": tune.grid_search([1e-4, 1e-3, 1e-2]),\n",
    "    \"epochs\": tune.grid_search([100, 300]),\n",
    "    \"data_path\": \"/Users/tozastation/ghq/github.com/tozastation/try-astronomy/auto_area_estimation/ceers.hdf5\",\n",
    "    # 重要と思われるパラメータに絞る\n",
    "    # \"use_mag_iso\": True,\n",
    "    # \"use_area_iso\": True,\n",
    "    # \"use_kron_radius\": True,\n",
    "    # \"use_npix\": True,\n",
    "    \"use_mag_iso\": tune.grid_search([True, False]),\n",
    "    \"use_area_iso\": tune.grid_search([True, False]),\n",
    "    \"use_kron_radius\": tune.grid_search([True, False]),\n",
    "    \"use_npix\": tune.grid_search([True, False]),\n",
    "    # 他のパラメータは一旦固定値にするか、探索から外す\n",
    "    \"use_xmax\": False,\n",
    "    \"use_xmin\": False,\n",
    "    \"use_ymax\": False,\n",
    "    \"use_ymin\": False,\n",
    "    \"use_x2_image\": False,\n",
    "    \"use_y2_image\": False,\n",
    "    \"use_xy_image\": False,\n",
    "    \"use_flux_auto\": False,\n",
    "    \"use_flux_iso\": False,\n",
    "    \"use_flux_aper_0\": False,\n",
    "    \"use_flux_aper_1\": False,\n",
    "    \"use_flux_aper_2\": False,\n",
    "    \"use_flux_radius\": False,\n",
    "    \"use_flux_radius_20\": False,\n",
    "    \"use_flux_radius_90\": False,\n",
    "}\n",
    "\n",
    "# スケジューラを設定 (リソースを効率的に利用するために ASHA を使用)\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    reduction_factor=2,\n",
    "    max_t=500  # 最大エポック数を設定 (config の最大値に合わせる)\n",
    ")\n",
    "\n",
    "# Ray Tune のトレーニング関数にリソースを指定\n",
    "trainable_with_resources = tune.with_resources(trainable=train_tune, resources={\"cpu\": 2, \"gpu\": 0})\n",
    "\n",
    "# Ray Tune の実行\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_resources,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=scheduler,\n",
    "        # Grid Search を使用して全ての組み合わせを探索\n",
    "        search_alg=tune.search.basic_variant.BasicVariantGenerator(),\n",
    "        num_samples=1,  # サンプル数を指定\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        # prefix + suffix =<\\ctrl3348>-MM-DD_HH-MM-SS\n",
    "        name=\"linear_regression_tune-\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "    )\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "# ベストなハイパーパラメータを取得\n",
    "best_trial = results.get_best_result(\"best_loss\", \"min\", filter_nan_and_inf=False) # best_loss を指標とする\n",
    "print(f\"Best Trial: {best_trial.metrics}\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.metrics['best_loss']:.4f}\")\n",
    "\n",
    "# ベストモデルの評価と検証\n",
    "\n",
    "# ベストなハイパーパラメータを使用してモデルを評価する関数\n",
    "def evaluate_best_model(best_config, data_path):\n",
    "    # データセットの読み込み\n",
    "    dataset = HDF5Dataset(data_path, features + [target], None)\n",
    "    dataloader = torch.utils.data.DataLoader( # DataLoader を使用\n",
    "        dataset,\n",
    "        batch_size=len(dataset),\n",
    "        shuffle=False, # 評価時はシャッフルしない\n",
    "    )\n",
    "    for batch in dataloader:\n",
    "        data = {key: batch[key].numpy() for key in features + [target]}\n",
    "        break\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # ベストな設定に基づいて使用する特徴量のリストを作成\n",
    "    selected_features = []\n",
    "    selected_features.append('mag_auto')\n",
    "    if best_config[\"use_mag_iso\"]:\n",
    "        selected_features.append('mag_iso')\n",
    "    if best_config[\"use_area_iso\"]:\n",
    "        selected_features.append('area_iso')\n",
    "    if best_config[\"use_kron_radius\"]:\n",
    "        selected_features.append('kron_radius')\n",
    "    if best_config[\"use_npix\"]:\n",
    "        selected_features.append('npix')\n",
    "    if best_config[\"use_xmax\"]:\n",
    "        selected_features.append('xmax')\n",
    "    if best_config[\"use_xmin\"]:\n",
    "        selected_features.append('xmin')\n",
    "    if best_config[\"use_ymax\"]:\n",
    "        selected_features.append('ymax')\n",
    "    if best_config[\"use_ymin\"]:\n",
    "        selected_features.append('ymin')\n",
    "    if best_config[\"use_x2_image\"]:\n",
    "        selected_features.append('x2_image')\n",
    "    if best_config[\"use_y2_image\"]:\n",
    "        selected_features.append('y2_image')\n",
    "    if best_config[\"use_xy_image\"]:\n",
    "        selected_features.append('xy_image')\n",
    "    if best_config[\"use_flux_auto\"]:\n",
    "        selected_features.append('flux_auto')\n",
    "    if best_config[\"use_flux_iso\"]:\n",
    "        selected_features.append('flux_iso')\n",
    "    if best_config[\"use_flux_aper_0\"]:\n",
    "        selected_features.append('flux_aper_0')\n",
    "    if best_config[\"use_flux_aper_1\"]:\n",
    "        selected_features.append('flux_aper_1')\n",
    "    if best_config[\"use_flux_aper_2\"]:\n",
    "        selected_features.append('flux_aper_2')\n",
    "    if best_config[\"use_flux_radius\"]:\n",
    "        selected_features.append('flux_radius')\n",
    "    if best_config[\"use_flux_radius_20\"]:\n",
    "        selected_features.append('flux_radius_20')\n",
    "    if best_config[\"use_flux_radius_90\"]:\n",
    "        selected_features.append('flux_radius_90')\n",
    "\n",
    "    X = df[selected_features].values\n",
    "    y = df[target].values\n",
    "    # データの分割 (トレーニング用とテスト用データに分割。Ray Tune の train 関数と同じ分割方法を使用)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # データの標準化 (トレーニングデータでfitしたscalerをテストデータにも適用)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # NumPy 配列を PyTorch の Tensor に変換\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "\n",
    "    # モデルのインスタンスを作成 (入力サイズは選択された特徴量の数)\n",
    "    input_size = len(selected_features)\n",
    "    output_size = 1\n",
    "    best_model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "    # 損失関数を定義\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 学習済みモデルのロード (Ray Tune は通常、checkpoint を保存するため、そこからロードするのが一般的ですが、\n",
    "    # 今回は簡単な線形回帰モデルなので、学習済みのパラメータを直接適用します)\n",
    "    # 注意: Ray Tune は Trial のディレクトリに checkpoint を保存します。\n",
    "    #       通常は best_trial.checkpoint.to_dict()[\"state\"][\"model\"] のようにしてロードします。\n",
    "    #       しかし、今回の train_tune 関数では明示的に保存処理を書いていないため、\n",
    "    #       ここではベストな設定で再度学習させるか、最後のモデルの状態を利用します。\n",
    "    #       簡単な例として、ここではベストな設定で再度学習させることにします。\n",
    "\n",
    "    # モデルの再学習 (ベストなハイパーパラメータを使用)\n",
    "    optimizer = torch.optim.Adam(best_model.parameters(), lr=best_config[\"learning_rate\"])\n",
    "    for epoch in range(best_config[\"epochs\"]):\n",
    "        best_model.train() # モデルを訓練モードに設定\n",
    "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "        outputs = best_model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{best_config['epochs']}], Loss: {loss.item():.4f}\", end='\\r')\n",
    "    print(\"\\nBest model re-trained.\")\n",
    "\n",
    "    # モデルを評価モードに設定\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # テストデータで予測\n",
    "        test_outputs = best_model(X_test_tensor)\n",
    "        # 損失を計算\n",
    "        test_loss = criterion(test_outputs, y_test_tensor)\n",
    "        # 評価指標を計算 (NumPy 配列に変換)\n",
    "        y_true = y_test_tensor.cpu().numpy()\n",
    "        y_pred = test_outputs.cpu().numpy()\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\n--- Best Model Evaluation ---\")\n",
    "    print(f\"Test Loss (MSE): {test_loss.item():.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "    # 予測値と真の値の散布図を作成する関数\n",
    "    def plot_predictions(y_true, y_pred):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(y_true, y_pred)\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(\"True Values vs. Predicted Values (Best Model)\")\n",
    "        # y=x の線を追加 (理想的な予測を示す)\n",
    "        lims = [np.min([y_true, y_pred]), np.max([y_true, y_pred])]\n",
    "        plt.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "        plt.xlim(lims)\n",
    "        plt.ylim(lims)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # 散布図を表示\n",
    "    plot_predictions(y_true, y_pred)\n",
    "\n",
    "    # モデルの保存判定\n",
    "    previous_r2_file = \"previous_best_r2.txt\"\n",
    "    model_save_path = \"best_linear_regression_model.pth\"\n",
    "\n",
    "    try:\n",
    "        with open(previous_r2_file, \"r\") as f:\n",
    "            previous_best_r2 = float(f.readline())\n",
    "    except FileNotFoundError:\n",
    "        previous_best_r2 = -float('inf') # まだ記録がない場合は負の無限大とする\n",
    "\n",
    "    print(f\"\\nPrevious best R-squared: {previous_best_r2:.4f}\")\n",
    "    print(f\"Current R-squared: {r2:.4f}\")\n",
    "\n",
    "    if r2 > previous_best_r2:\n",
    "        torch.save(best_model.state_dict(), model_save_path)\n",
    "        with open(previous_r2_file, \"w\") as f:\n",
    "            f.write(str(r2))\n",
    "        print(f\"Model saved to {model_save_path}. Updated best R-squared to {r2:.4f} in {previous_r2_file}.\")\n",
    "    else:\n",
    "        print(\"Current model's performance is not better than the previous best. Model not saved.\")\n",
    "\n",
    "# 評価関数の実行\n",
    "evaluate_best_model(best_trial.config, config[\"data_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for ms-toolsai.jupyter:_builtin.jupyterServerUrlProvider:b5140927-1488-4614-9dc4-b349c1399864"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_dataset import HDF5Dataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time  # time モジュールをインポート\n",
    "\n",
    "# Ray Tune のためのインポート\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# デバイスの設定 (Ray Tune の設定で GPU を利用するかどうかを指定できるため、ここでは CPU に固定)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using CPU for Ray Tune.\")\n",
    "\n",
    "# 特徴量として使用する列名\n",
    "features = [\n",
    "    'x2_image',\n",
    "    'y2_image',\n",
    "    'cxx_image',\n",
    "    'cyy_image',\n",
    "    'fluxerr_auto',\n",
    "    'flux_radius',\n",
    "    'area_iso',\n",
    "    'xy_image',\n",
    "    'npix',\n",
    "    'flux_aper_1',\n",
    "]\n",
    "# ターゲット変数 (予測したい変数)\n",
    "target = 'area_auto'\n",
    "\n",
    "# 線形回帰モデルの定義 (PyTorch の nn.Module を継承してモデルを定義)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # 線形層 (全結合層) を定義。入力サイズと出力サイズを指定\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 順伝播処理。入力 x を線形層に通して出力を得る\n",
    "        return self.linear(x)\n",
    "\n",
    "# Ray Tune で最適化する train 関数\n",
    "def train_tune(config):\n",
    "    start_time = time.time()  # 学習開始時間を記録\n",
    "\n",
    "    # データセットの読み込み\n",
    "    dataset = HDF5Dataset(config[\"data_path\"], features + [target], None)\n",
    "    dataloader = torch.utils.data.DataLoader( # DataLoader を使用\n",
    "        dataset,\n",
    "        batch_size=len(dataset),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    for batch in dataloader:\n",
    "        data = {key: batch[key].numpy() for key in features + [target]}\n",
    "        break\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.dropna()\n",
    "    # config に基づいて使用する特徴量のリストを作成\n",
    "    selected_features = features.copy()  # features をコピーして使用\n",
    "    selected_features.append('mag_auto')\n",
    "\n",
    "    X = df[selected_features].values\n",
    "    y = df[target].values\n",
    "    # データの分割 (トレーニング用とテスト用データに分割。テストデータはモデルの性能評価に使用)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # データの標準化 (特徴量のスケールを揃えることで、モデルの学習を安定させ、性能を向上させる)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # NumPy 配列を PyTorch の Tensor に変換 (PyTorch のモデルで扱えるようにデータ形式を変換)\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "\n",
    "    # モデルのインスタンスを作成 (ハイパーパラメータを使用)\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = 1\n",
    "    model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "    # 損失関数とオプティマイザの定義 (ハイパーパラメータを使用)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    best_trial_loss = float('inf')\n",
    "\n",
    "    # トレーニングループ (ハイパーパラメータを使用)\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        # 順伝播 (forward pass) (モデルに入力を与えて予測値を出力)\n",
    "        outputs = model(X_train)\n",
    "        # 損失の計算 (モデルの予測値と正解ラベルとの誤差を計算)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        current_loss = loss.item()\n",
    "        # 逆伝播とパラメータ更新 (backward pass and optimization)\n",
    "        optimizer.zero_grad() # 勾配をゼロに初期化 (前のイテレーションで計算された勾配をリセット)\n",
    "        loss.backward() # 勾配の計算 (損失関数に基づいて、各パラメータの勾配を計算)\n",
    "        optimizer.step() # パラメータの更新 (計算された勾配に基づいて、モデルのパラメータを更新)\n",
    "\n",
    "        best_trial_loss = min(best_trial_loss, current_loss)\n",
    "\n",
    "        # Ray Tune に Loss を報告\n",
    "        print({\"loss\": current_loss, \"epoch\": epoch, \"best_loss\": best_trial_loss})\n",
    "        tune.report({\"loss\": current_loss, \"epoch\": epoch, \"best_loss\": best_trial_loss})\n",
    "\n",
    "    end_time = time.time()  # 学習終了時間を記録\n",
    "    duration = end_time - start_time\n",
    "    tune.report({\"loss\": current_loss, \"epoch\": epoch, \"best_loss\": best_trial_loss, \"training_duration\": duration}) # 学習時間を報告\n",
    "\n",
    "# ハイパーパラメータの探索空間を定義\n",
    "# TODO: The number of pre-generated samples (2621440) exceeds\n",
    "# 3 * 2 * 2 * 2 * 2 * 2 = 3 * 2 * 32 = 96 通り\n",
    "config = {\n",
    "    # モデルの学習は、損失関数（モデルの予測と実際のデータのずれを表す関数）の値を最小にするパラメータを見つけるプロセスです\n",
    "    \"learning_rate\": tune.grid_search([1e-4, 1e-3, 1e-2]),\n",
    "    \"epochs\": tune.grid_search([100, 300]),\n",
    "    \"data_path\": \"/Users/tozastation/ghq/github.com/tozastation/try-astronomy/auto_area_estimation/ceers.hdf5\",\n",
    "}\n",
    "\n",
    "# スケジューラを設定 (リソースを効率的に利用するために ASHA を使用)\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    reduction_factor=2,\n",
    "    max_t=500  # 最大エポック数を設定 (config の最大値に合わせる)\n",
    ")\n",
    "\n",
    "# Ray Tune のトレーニング関数にリソースを指定\n",
    "trainable_with_resources = tune.with_resources(trainable=train_tune, resources={\"cpu\": 2, \"gpu\": 0})\n",
    "\n",
    "# Ray Tune の実行\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_resources,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=scheduler,\n",
    "        # Grid Search を使用して全ての組み合わせを探索\n",
    "        search_alg=tune.search.basic_variant.BasicVariantGenerator(),\n",
    "        num_samples=1,  # サンプル数を指定\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        # prefix + suffix =<\\ctrl3348>-MM-DD_HH-MM-SS\n",
    "        name=\"linear_regression_tune-\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "    )\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "# ベストなハイパーパラメータを取得\n",
    "best_trial = results.get_best_result(\"best_loss\", \"min\", filter_nan_and_inf=False) # best_loss を指標とする\n",
    "print(f\"Best Trial: {best_trial.metrics}\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.metrics['best_loss']:.4f}\")\n",
    "\n",
    "# ベストモデルの評価と検証\n",
    "\n",
    "# ベストなハイパーパラメータを使用してモデルを評価する関数\n",
    "def evaluate_best_model(best_config, data_path, n_splits=5): # n_splits は分割数\n",
    "    # データセットの読み込み\n",
    "    dataset = HDF5Dataset(data_path, features + [target], None)\n",
    "    dataloader = torch.utils.data.DataLoader( # DataLoader を使用\n",
    "        dataset,\n",
    "        batch_size=len(dataset),\n",
    "        shuffle=False, # 評価時はシャッフルしない\n",
    "    )\n",
    "    for batch in dataloader:\n",
    "        data = {key: batch[key].numpy() for key in features + [target]}\n",
    "        break\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # ベストな設定に基づいて使用する特徴量のリストを作成\n",
    "    selected_features = features.copy()  # features をコピーして使用\n",
    "    selected_features.append('mag_auto')\n",
    "\n",
    "    X = df[selected_features].values\n",
    "    y = df[target].values\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    print(\"\\n--- Cross-Validation Evaluation ---\")\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # データの標準化 (各フォールドごとに独立して行う)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # NumPy 配列を PyTorch の Tensor に変換\n",
    "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "\n",
    "        # モデルのインスタンスを作成\n",
    "        input_size = len(selected_features)\n",
    "        output_size = 1\n",
    "        model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "        # 損失関数とオプティマイザの定義\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_config[\"learning_rate\"])\n",
    "\n",
    "        # モデルの学習\n",
    "        for epoch in range(best_config[\"epochs\"]):\n",
    "            model.train()\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{best_config['epochs']}], Loss: {loss.item():.4f}\", end='\\r')\n",
    "        print(\"\\nFold training finished.\")\n",
    "\n",
    "        # モデルの評価\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            y_true_fold = y_val_tensor.cpu().numpy()\n",
    "            y_pred_fold = val_outputs.cpu().numpy()\n",
    "            mse = mean_squared_error(y_true_fold, y_pred_fold)\n",
    "            mae = mean_absolute_error(y_true_fold, y_pred_fold)\n",
    "            r2 = r2_score(y_true_fold, y_pred_fold)\n",
    "            mse_scores.append(mse)\n",
    "            mae_scores.append(mae)\n",
    "            r2_scores.append(r2)\n",
    "            all_y_true.extend(y_true_fold)\n",
    "            all_y_pred.extend(y_pred_fold)\n",
    "\n",
    "        print(f\"Fold {fold+1} Validation Loss (MSE): {val_loss.item():.4f}\")\n",
    "        print(f\"Fold {fold+1} Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Fold {fold+1} Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"Fold {fold+1} R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "    # 平均評価指標の計算\n",
    "    mean_mse = np.mean(mse_scores)\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "\n",
    "    print(\"\\n--- Average Cross-Validation Results ---\")\n",
    "    print(f\"Average Mean Squared Error (MSE): {mean_mse:.4f}\")\n",
    "    print(f\"Average Mean Absolute Error (MAE): {mean_mae:.4f}\")\n",
    "    print(f\"Average R-squared (R2): {mean_r2:.4f}\")\n",
    "\n",
    "    # 予測値と真の値の散布図を作成\n",
    "    def plot_predictions(y_true, y_pred):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(y_true, y_pred)\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(\"True Values vs. Predicted Values (Best Model - Cross-Validation)\")\n",
    "        lims = [np.min([y_true, y_pred]), np.max([y_true, y_pred])]\n",
    "        plt.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "        plt.xlim(lims)\n",
    "        plt.ylim(lims)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    plot_predictions(np.array(all_y_true), np.array(all_y_pred))\n",
    "\n",
    "    # モデルの保存判定 (平均 R2 スコアを使用)\n",
    "    previous_r2_file = \"previous_best_r2.txt\"\n",
    "    model_save_path = \"best_linear_regression_model.pth\"\n",
    "\n",
    "    try:\n",
    "        with open(previous_r2_file, \"r\") as f:\n",
    "            previous_best_r2 = float(f.readline())\n",
    "    except FileNotFoundError:\n",
    "        previous_best_r2 = -float('inf')\n",
    "\n",
    "    print(f\"\\nPrevious best R-squared: {previous_best_r2:.4f}\")\n",
    "    print(f\"Current average R-squared: {mean_r2:.4f}\")\n",
    "\n",
    "    # 各フォールドで最も良いモデルを保存するか、平均で判断するかなど、保存戦略を検討する必要があります。\n",
    "    # ここでは、平均 R2 スコアが改善した場合にモデルを保存する例を示します。\n",
    "    if mean_r2 > previous_best_r2:\n",
    "        # どのモデルを保存するかは戦略によります。ここでは、最後に学習したモデルを保存します。\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        with open(previous_r2_file, \"w\") as f:\n",
    "            f.write(str(mean_r2))\n",
    "        print(f\"Model saved to {model_save_path}. Updated best average R-squared to {mean_r2:.4f} in {previous_r2_file}.\")\n",
    "    else:\n",
    "        print(\"Current model's average performance is not better than the previous best. Model not saved.\")\n",
    "\n",
    "# 評価関数の実行 (交差検証の分割数を指定)\n",
    "evaluate_best_model(best_trial.config, config[\"data_path\"], n_splits=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
